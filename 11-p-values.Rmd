# P-values {#pvalues}

```{r setup-pvalues, include=FALSE, purl=FALSE}
chap <- 11
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**
knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  fig.align='center',
  warning = FALSE
  )
options(scipen = 99, digits = 3)
# Set random number generator see value for replicable pseudorandomness. 
set.seed(2018)
```

In Chapter \@ref(CIs), we covered how to construct and interpret confidence intervals, which use the theory of repeated samples to make inferences from a sample (your data) to a population. To do so, we used counterfactual thinking that underpins statistical reasoning, wherein making inferences requires you to imagine alternative versions of your data that you might have under other possible samples selected in the same way. In this chapter, we extend this counterfactual reasoning to imagine other possible samples you might have seen if you knew the trend in the population. This way of thinking will lead us to define p-values.

### Needed Packages {-}

Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(moderndive)
library(infer)
library(ggplot2movies)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(readr)
library(knitr)
library(kableExtra)
library(patchwork)
library(scales)
```

## Stochastic Proof by Contradiction {#proof-by-contradiction}
In many scientific pursuits, the goal is not simply to estimate a population parameter. Instead, the goal is often to understand if there is a difference between two groups in the population or if there is a relationship between two (or more) variables in the population. For example, we might want to know if average SAT scores differ between men and women, or if there is a relationship between education and income in the population in the United States. 

Let’s take the difference in means between two groups as a motivating example. In order to prove that there is a difference between average SAT scores for men and women, we might proceed with what is in math called a proof by contradiction. Here, however, this proof is probabilistic (aka stochastic).

**Stochastic Proof by Contraction**:

There are three steps in a Proof by Contradiction. In order to illustrate these, assume we wish to prove that there is a relationship between X and Y.

1.	Negate the conclusion: Begin by assuming the opposite – that there is no relationship between X and Y.
2.	Analyze the consequences of this premise:  If there is no relationship between X and Y in the population, what would the sampling distribution of the estimate of the relationship between X and Y look like?
3.	Look for a contradiction: Compare the relationship between X and Y observed in your sample to this sampling distribution. How (un)likely is this observed relationship? 

If likelihood of the observed relationship is small (given your assumption of no relationship), then this is evidence that there is in fact a relationship between X and Y in the population.

## Repeated samples, the null hypothesis, and p-values

### Null hypothesis

In the example of asking if there is a difference in SAT scores between men and women, you will note that in order to prove that there is a difference, we begin by assuming that there is not a difference (Step 1). We call this the null hypothesis – it is the hypothesis we are attempting to disprove. The most common null hypotheses are:

* A parameter is 0 in the population.
* There is no difference between two or more groups in the population.
* There is no relationship between two variables in the population.

Importantly, this hypothesis is about the value or relationship in the population, not the sample. (This is a very easy mistake to make). Remember, you have data in your sample, so you know without a doubt if there is a difference or relationship in your data (that is your estimate). What you do not know is if there is a difference or relationship in the population. 
Once a null hypothesis is determined, the next step is to determine what the sampling distribution of the estimator would be if this null hypothesis were true (Step 2). We can determine what this null distribution would look like, just as we've done with sampling distributions more generally: using mathematical theory. 

### P-values
Once the distribution of the sample statistic is determined under the null hypothesis, to complete the stochastic proof by contradiction, you simply need to ask: Given this distribution, how likely is it that I would have drawn a random sample in which the estimated value is this extreme or more extreme? 

This is the **p-value**: The probability of your observing an estimate as extreme as the one you observed if the null hypothesis is true. If this p-value is small, it means that this data is unlikely to occur under the null hypothesis, and thus the null hypothesis is unlikely to be true. (See, proof by contradiction!)

```{r p-value-diagram, echo=FALSE, out.width='100%', fig.cap="P-value diagram"}
knitr::include_graphics('images/p-value-figure.png')
```

<!-- retrieved diagram from https://scientistseessquirrel.wordpress.com/2015/02/09/in-defence-of-the-p-value/  need to cite?-->

In general, in order to estimate a p-value, you first need to standardize your sample statistic. This standardization makes it easier to determine the sampling distribution under the null hypothesis. 

Standardization is conducted using the following formula:

$$t\_stat = \frac{Estimate - Null \ \ value}{SE(Estimate)}$$

Note this is just a special case of the previous standardization formula we've seen before, where here we're plugging in the "null value" for the mean of the estimate. The null value refers to the value of the population parameter assumed by the null hypothesis. As we mentioned, in many cases the null value is zero. That is, we begin the proof by contradiction by assuming there is no relationship, no differences between groups, etc. in the population.

This standardized **t-statistic** is then used to determine the sampling distribution under the null hypothesis and the p-value based upon the observed value. 

## P-value and Null Distribution Example

### IMDB data {#imdb}
The `movies` dataset in the `ggplot2movies` package contains information on `r nrow(movies) %>% comma()` movies that have been rated by users of IMDB.com. 

```{r}
movies
```

We'll focus on a random sample of 68 movies that are classified as either "action" or "romance" movies but not both. We disregard movies that are classified as both so that we can assign all 68 movies into either category. Furthermore, since the original `movies` dataset was a little messy, we provided a pre-wrangled version of our data in the `movies_sample` data frame included in the `moderndive` package (you can look at the code to do this data wrangling [here](https://github.com/moderndive/moderndive/blob/master/data-raw/process_data_sets.R#L14)):

```{r}
movies_sample
```

The variables include the `title` and `year` the movie was filmed. Furthermore, we have a numerical variable `rating`, which is the IMDB rating out of 10 stars, and a binary categorical variable `genre` indicating if the movie was an `Action` or `Romance` movie. We are interested in whether there is a difference in average ratings between the `Action` and `Romance` genres. That is, our parameter of interest is $\mu_1 - \mu_2$, which we estimate by $\bar{x}_1 - \bar{x}_2$.

Let's calculate the number of movies (`n`), the mean rating (`xbar`), and the standard deviation (`s`) split by the binary variable `genre`. We'll also calculate $\frac{s_i^2}{n_i}$ for each group (`var_xbar`), which will be used to calculate the standard error for $\bar{x}_1 - \bar{x}_2$.

```{r}
genre_mean_stats <- movies_sample %>% 
  group_by(genre) %>% 
  summarize(n = n(), 
            xbar = mean(rating), 
            s = sd(rating),
            var_xbar = s^2/n)
genre_mean_stats
```

We start by assuming there is no difference, therefore our null value is $\mu_1 - \mu_2 = 0$. We want to calculate the t-statistic for this scenario:

$$t\_stat = \frac{Estimate - Null \ \ value}{SE(Estimate)} = \frac{(\bar{x}_1 - \bar{x}_2) - 0}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$ 
Let's compute all the necessary values from our sample data. 
```{r, echo=FALSE}
movies_genre_summaries <- movies_sample %>% 
  group_by(genre) %>% 
  summarize(n = n(), mean_rating = mean(rating), std_dev = sd(rating))
x_bar_action <- movies_genre_summaries %>% 
  filter(genre == "Action") %>% 
  pull(mean_rating)
x_bar_romance <- movies_genre_summaries %>% 
  filter(genre == "Romance") %>% 
  pull(mean_rating)
sd_action <- movies_genre_summaries %>% 
  filter(genre == "Action") %>% 
  pull(std_dev)
sd_romance <- movies_genre_summaries %>% 
  filter(genre == "Romance") %>% 
  pull(std_dev)
n_action <- movies_genre_summaries %>% 
  filter(genre == "Action") %>% 
  pull(n)
n_romance <- movies_genre_summaries %>% 
  filter(genre == "Romance") %>% 
  pull(n)
se_diff <- sqrt(sd_romance^2 / n_romance + sd_action^2 / n_action)
t_stat <- ((x_bar_romance - x_bar_action) - 0)/sqrt(sd_romance^2 / n_romance + sd_action^2 / n_action)
```



So we have `r n_romance` movies with an average rating of `r x_bar_romance %>% round(2)` stars out of 10 and `r n_action` movies with a sample mean rating of `r x_bar_action %>% round(2)` stars out of 10. The difference in these average ratings is thus `r x_bar_romance %>% round(2)` - `r x_bar_action %>% round(2)` = `r (x_bar_romance - x_bar_action) %>% round(2)`. And the standard error of this difference is `r se_diff`.

```{r}
diff_genre_means_stats <- genre_mean_stats %>% 
  summarize(diff_in_means = diff(xbar),
            SE_diff = sqrt(sum(var_xbar))) 
diff_genre_means_stats
```

Our resulting `t_stat` is `r t_stat`.

```{r}
diff_genre_means_stats %>% 
  summarise(t_stat = diff_in_means / SE_diff)
```

There appears to be an edge of `r (x_bar_romance - x_bar_action) %>% round(2)` stars in romance movie ratings. The question is however, are these results indicative of a true difference for all romance and action movies? Or could this difference be attributable to chance and sampling variation? Computing a p-value for this t-statistic can help us to answer this. 

### Theory based p-values
Recall from Chapter \@ref(CIs) that even though the sampling distribution of many estimators are normally distributed, the t-statistic computed above often follows a $t(df)$ distribution because the formula involves an additional estimated quantity $s^2$ when the population variance is unknown. Recall that (difference in) proportions still follow the $N(0,1)$ distribution because they do not require $s^2$ to be estimated. An abbreviated version of Table \@ref(tab:samp-dist-table-ch-10) with the relevant degrees of freedom is given below


Statistics |Population parameter |		Estimator	|	t-distribution df
-----------|---------------------|--------------|------------------
Mean       |$\mu$                |	$\bar{x}$		|	$n – 1$
Difference in means | $\mu_1 -\mu_2$	|	$\bar{x}_1 - \bar{x}_2$ | $min(n_1 -1, n_2 – 1)$ 
Regression intercept | $\beta_0$ | $b_0$ | $n - 2$
Regression slope | $\beta_1$ | $b_1$ | $n - 2$


We can calculate a p-value by asking: Assuming the null distribution, what is the probability that we will see a t-statistic as extreme as the one from our data? To answer this, we can calculate this using `pt(t_stat, df)`, where `t_stat` is the t-statistic we calculated from our sample data, and `df` is the appropriate degrees of freedom for our estimator and sample size. Remember there is a default argument `lower.tail = TRUE` in the `pt()` function, which means it returns the probability *to the left* of the `t_stat` value you enter. For example, Figure \@ref(fig:t-lower-tail) shows the shaded probabilty that is implied by the code `pt(1.5, df = 30)` and Figure \@ref(fig:t-upper-tail) shows the shaded probabilty that is implied by the code `pt(1.5, df = 30, lower.tail = FALSE)`. 

```{r t-lower-tail, fig.cap = "pt(1.5, df = 30, lower.tail = TRUE)", message = FALSE, echo = FALSE, warning = FALSE}
shade_curve <- function(MyDF, tstart, tend, fill = "red", alpha = .5){
  geom_area(data = subset(MyDF, x >= 0 + tstart
                          & x < 0 + tend),
            aes(y=y), fill = fill, color = NA, alpha = alpha)
}

data <- data.frame(x = seq(from = -5, to = 5, by = 0.01)) %>% 
  mutate(y = dnorm(x))


ggplot(data.frame(x = c(-6, 6)), aes(x = x)) +
    stat_function(fun = dt, args = list(df = 66),
                    aes(colour = "t-distribution"), size = 1) +
  geom_vline(xintercept = 1.5, linetype = 2) +
  shade_curve(data, tstart = -5, tend = 1.5)
  
```

```{r t-upper-tail, fig.cap = "pt(1.5, df = 30, lower.tail = FALSE)", message = FALSE, echo = FALSE, warning = FALSE}
ggplot(data.frame(x = c(-6, 6)), aes(x = x)) +
    stat_function(fun = dt, args = list(df = 66),
                    aes(colour = "t-distribution"), size = 1) +
  geom_vline(xintercept = 1.5, linetype = 2) +
  shade_curve(data, tstart = 1.5, tend = 5)
  
```
**Caveat**:
It is important to note that the t-distribution is often referred to as a “small sample” distribution. That is because once the degrees of freedom are large enough (when the sample size is large), the t-distribution is actually quite similar to the normal distribution as we have seen previously. For analysis purposes, however, you don’t need to determine when to use one or the other as your sampling distribution: just always use the t-distribution (unless dealing with proportions). 


In our IMDB movies example, we observe a `t_stat = 2.91`, and we want to know what the probability of observing a t-statistic *as large* as this would be under the null distribution. Therefore we want to include the argument `lower.tail = FALSE` when computing our p-value. Note our $df = n_1 + n_2 - 2 = 36 + 32 - 2 = 66$, so our p-value is given by `pt(2.91, 66, lower.tail = FALSE)` = `r round(pt(2.91, 66, lower.tail = FALSE), 4)`. This tells us that if the null distribution is true (i.e. there is no true difference between average ratings of romance and action movies on IMDB), we would only observe a difference as large as we did `r round(pt(2.91, 66, lower.tail = FALSE)*100, 2)`% of the time. This provides evidence - via proof by contradiction - that the null distribution is likely false; that is, there is likely a true difference in average ratings of romance and action moview on IMDB.

<!-- ## CDC Example -->

<!-- ```{r} -->
<!-- source("http://www.openintro.org/stat/data/cdc.R") -->

<!-- prop_poor_health_stats <- cdc %>%  -->
<!--   group_by(smoke100) %>%  -->
<!--   summarize(n = n(), -->
<!--             num_fair_poor = sum(genhlth == "fair" | genhlth == "poor"), -->
<!--             pi_hat = num_fair_poor/n, -->
<!--             var_pi_hat = pi_hat*(1-pi_hat)/num_fair_poor) -->
<!-- prop_poor_health_stats -->

<!-- diff_in_props <- prop_poor_health_stats %>%  -->
<!--   summarize(diff_in_props = diff(pi_hat), -->
<!--             SE_diff = sqrt(sum(var_pi_hat)), -->
<!--             t_stat = (diff_in_props - 0)/SE_diff) -->
<!-- diff_in_props -->

<!-- pnorm(4.32, lower.tail = FALSE) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ed_data <- read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vTQ9AvbzZ2DBIRmh5h_NJLpC_b4u8-bwTeeMxwSbGX22eBkKDt7JWMqnuBpAVad6-OXteFcjBY4dGqf/pub?gid=300215043&single=true&output=csv") -->

<!-- stats <- ed_data %>%  -->
<!--   summarize(n = n(), -->
<!--             sy = sd(MATH_pre), -->
<!--             sx = sd(SES_CONT), -->
<!--             SE = sqrt(sy^2/(sx^2*(n-1)))) -->

<!-- ed_fit <- lm(data = ed_data, MATH_pre ~ SES_CONT) -->
<!-- summary(ed_fit) -->
<!-- ``` -->


## Interpretation of P-values
Like many statistical concepts, p-values are often misunderstood and misinterpreted. Remember, a p-value is the probability that you would observe data as extreme as the data you do if, in fact, the null hypothesis is true. As Wikipedia notes:

* The p-value is not the probability that the null hypothesis is true, or the probability that the alternative hypothesis is false.
* The p-value is not the probability that the observed effects were produced by random chance alone.
* The p-value does not indicate the size or importance of the observed effect.

Finally, remember that the p-value is a probabilistic attempt at making a proof by contradiction. Unlike in math, this is not a definitive proof. For example, if the p-value is 0.10, this means that if the null hypothesis is true, there is a 10% chance that you would observe an effect as large as the one in your sample. Depending upon if you are a glass-half-empty or glass-half-full kind of person, this could be seen as large or small:

* “Only 10% chance is small, which is unlikely. This must mean that the null hypothesis is not true,” or 
* “But we don’t know that for sure: in 10% of possible samples, this does occur just by chance. The null hypothesis could be true.”

This will be important to keep in mind as we move towards using p-values for decision making in Chapter \@ref(hypothesis-tests).

