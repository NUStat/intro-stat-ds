# (PART) Statistical Theory {-} 

# Randomization and Causality {#causality}

```{r setup-causality, include=FALSE, purl=FALSE}
chap <- 13
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**
knitr::opts_chunk$set(
  tidy = FALSE,
  out.width = '\\textwidth',
  fig.height = 4,
  fig.align='center',
  warning = FALSE
  )
options(scipen = 99, digits = 3)
# Set random number generator see value for replicable pseudorandomness.
set.seed(2018)
```

In this chapter we kick off the third segment of this book: statistical theory. Up until this point, we have focused only on descriptive statistics and exploring the data we have in hand. Very often the data available to us is **observational data** – data that is collected via a survey in which nothing is manipulated or via a log of data (e.g., scraped from the web). As a result, any relationship we observe is limited to our specific sample of data, and the relationships are considered **associational**. In this chapter we introduce the idea of making inferences through a discussion of **causality** and **randomization**.


### Needed Packages {-}

Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section \@ref(packages) for information on how to install and load R packages.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(randomizr)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(readr)
library(knitr)
library(kableExtra)
library(patchwork)
library(scales)
```

## Causal Questions {#causal-questions}

What if we wanted to understand not just if X is associated with Y, but if X **causes** Y? Examples of causal questions include:

*	Does *smoking* cause *cancer*?
*	Do *after school programs* improve student *test scores*?
*	Does *exercise* make people *happier*?
*	Does exposure to *abstinence only education* lead to lower *pregnancy rates*?
*	Does *breastfeeding* increase baby *IQs*?

Importantly, note that while these are all causal questions, they do not all directly use the word *cause*. Other words that imply causality include:

* Improve
* Increase / decrease
* Lead to
* Make

In general, the tell-tale sign that a question is causal is if the analysis is used to make an argument for changing a procedure, policy, or practice. 

## Randomized experiments {#randomized-experiments}

The gold standard for understanding causality is the **randomized experiment**. For the sake of this chapter, we will focus on experiments in which people are randomized to one of two conditions: treatment or control. Note, however, that this is just one scenario; for example, schools, offices, countries, states, households, animals, cars, etc. can all be randomized as well, and can be randomized to more than two conditions. 

What do we mean by random? Be careful here, as the word “random” is used colloquially differently than it is statistically. When we use the word **random** in this context, we mean:

* Every person (or unit) has some chance (i.e., a non-zero probability) of being selected into the treatment or control group.
* The selection is based upon a **random process** (e.g., names out of a hat, a random number generator, rolls of dice, etc.)

In practice, a randomized experiment involves several steps. 

1.	Half of the sample of people is randomly assigned to the treatment group (T), and the other half is assigned to the control group (C). 
2.	Those in the treatment group receive a treatment (e.g., a drug) and those in the control group receive something else (e.g., business as usual, a placebo). 
3.	Outcomes (Y) in the two groups are observed for all people.
4.	The effect of the treatment is calculated using a simple regression model,
$$\hat{y} = b_0 + b_1T $$
where $T$ equals 1 when the individual is in the treatment group and 0 when they are in the control group. Note that using the notation introduced in Section \@ref(model2table), this would be the same as writing $\hat{y} = b_0 + b_1\mathbb{1}_{\mbox{Trt}}(x)$. We will stick with the $T$ notation for now, because this is more common in randomized experiments in practice. 

For this simple regression model, $b_1 = \bar{y}_T - \bar{y}_C$ is the observed "treatment effect", where $\bar{y}_T$ is the average of the outcomes in the treatment group and $\bar{y}_C$ is the average in the control group. This means that the "treatment effect" is simply the difference between the treatment and control group averages. 

### Random processes in R

There are several functions in R that mimic random processes. You have already seen one example in Chapters \@ref(regression) and \@ref(multiple-regression) when we used `sample_n` to randomly select a specifized number of rows from a dataset. The function `rbernoulli()` is another example, which allows us to mimic the results of a series of random coin flips. The first argument `n` specifies the number of trials (in this case, coin flips), and `p` specifies the probability of success for each trial. In our coin flip example, we can define success to be when the coin lands on heads. If we're using a fair coin then `p = 0.5`. 

Sometimes a random process can give results that don't *look* random. For example, even though any given coin flip has a 50% chance of landing on heads, it's possible to observe many tails in a row, just due to chance. In the example below, 10 coin flips resulted in only 3 heads, and the first 6 flips were tails. Note that TRUE corresponds to the notion of "success", so here TRUE = heads and FALSE = tails. 
```{r, echo = FALSE}
set.seed(2018)
```
```{r}
coin_flips <- rbernoulli(n = 10, p = 0.5)
coin_flips
```
Importantly, just because the results don't *look* random, does not mean that the results *aren't* random. If we were to repeat this random process, we will get a different set of random results. 
```{r, echo = FALSE}
set.seed(2019)
```
```{r}
coin_flips2 <- rbernoulli(n = 10, p = 0.5)
coin_flips2
```

Random processes can appear unstable, particularly if they are done only a small number of times (e.g. only 10 coin flips), but if we were to conduct the coin flip procedure thousands of times, we would expect the results to stabilize and see on average 50% heads.

```{r, echo = FALSE}
set.seed(437)
```

```{r}
coin_flips3 <- rbernoulli(n = 100000, p = 0.5)
coin_flips3 %>% 
  as.tibble() %>% 
  count(value) %>% 
  mutate(percent = n/sum(n))
```

Often times when running a randomized experiment in practice, you want to ensure that exactly half of your participants end up in the treatment group. In this case, you don't want to flip a coin for each participant, because just by chance, you could end up with 63% of people in the treatment group, for example. Instead, you can imagine each participant having an ID number, which is then randomly sorted or shuffled. You could then assign the first half of the randomly sorted ID numbers to the treatment group, for example. R has many ways of mimicing this type of process as well, such as the `randomizr` package.

## Omitted variables {#omitted-variables}

In a randomized experiment, we showed in Section \@ref(randomized-experiments) that we can calculate the estimated causal effect ($b_1$) of a treatment using a simple regression model. 

Why can’t we use the same model to determine causality with observational data? Recall our discussion from Section \@ref(correlation-is-not-causation). We have to be very careful not to make unwarranted causal claims from observational data, because there may be an **omitted variable** (Z), also known as a **confounder**:

![](confounder.png)

Here are some examples:

* There is a positive relationship between sales of ice cream (X) from street vendors and crime (Y). Does this mean that eating ice cream causes increased crime? No. The omitted variable is the season and weather (Z). That is, there is a positive relationship between warm weather (Z) and ice cream consumption (X) and between warm weather (Z) and crime (Y).
* Students that play an instrument (X) have higher grades (Y) than those that do not. Does this mean that playing an instrument causes improved academic outcomes? No. Some omitted variables here could be family socio-economic status and student motivation. That is, there is a positive relationship between student motivation (and a family with resources) (Z) and likelihood of playing an instrument (X) and between motivation / resources and student grades (Y). 
* Countries that eat a lot of chocolate (X) also win the most Nobel Prizes (Y). Does this mean that higher chocolate consumption leads to more Nobel Prizes? No. The omitted variable here is a country's wealth (Z). Wealthier countries win more Nobel Prizes and also consume more chocolate. 

Examples of associations that are misinterpreted as causal relationships abound. To see more examples, check out this website: https://www.tylervigen.com/spurious-correlations. 

## The magic of randomization

If omitted variables / confounders are such a threat to determining causality in observational data, why aren’t they also a threat in randomized experiments?

The answer is simple: **randomization**. Because people are randomized to treatment and control groups, on average there is no difference between these two groups on any characteristics *other than their treatment*. 

This means that before the treatment is given, on average the two groups (T and C) are equivalent to one another on every observed *and* unobserved variable. For example, the two groups should be similar in all **pre-treatment** variables: age, gender, motivation levels, heart disease, math ability, etc. Thus, when the treatment is assigned and implemented, any differences between outcomes *can be attributed to the treatment*. 

### Randomization Example {#ed-data}

Let’s see the magic of randomization in action. First, we'll load in a dataset called `ed_data`. This data originally came from the Early Childhood Longitudinal Study (ECLS) program but has been adapted for this example. Let's take a look at the data.
```{r eclsK, echo=FALSE, message=FALSE}
if(!file.exists("data/ed_data.csv")){
  ed_data <- "https://docs.google.com/spreadsheets/d/e/2PACX-1vRJbOGxTyRvSwYijZEz87DnWXdwyxRTNlC3teg4FyqZ7IFyeg2f9ksVJXxODawJast8egjO_IG5K2hO/pub?gid=2078077477&single=true&output=csv" %>%
    read_csv(na = "")
    write_csv(ed_data, "data/ed_data.csv")
} else {
  ed_data <- read_csv("data/ed_data.csv")
}
```

```{r, message = FALSE}
ed_data <- read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vRJbOGxTyRvSwYijZEz87DnWXdwyxRTNlC3teg4FyqZ7IFyeg2f9ksVJXxODawJast8egjO_IG5K2hO/pub?gid=2078077477&single=true&output=csv")

glimpse(ed_data)
```
It includes information on 335 Kindergarten students: indicator variables for whether they are female or minority students, information on their parents' highest level of education, a continuous measure of the their socio-economic status (SES), and their reading and math scores. For our purposes, we will assume that these are all **pre-treatment variables** that are measured on students at the beginning of the year, before we conduct our (hypothetical) randomized experiment. 

Imagine that we have a promising new curriculum for teaching math to Kindergarteners, and we want to know whether or not the curriculum is effective. We can conduct a randomized experiment to test this. We randomly assign half of the students to the treatment group, where they will receive the new curriculum, and the other half of students to the control group where they will receive the "business as usual" curriculum. We can accomplish this in R using the `complete_ra()` function in the `randomizr` package. 

Remember that because the treatment assignment was **random**, we don't expect a student's treatment status to be correlated with any of their other pre-treatment characteristics. In other words, students in the treatment and control groups should look approximately the same *on average*. Looking at the means of all the numeric variables by treatment group, we can see that this is true in our example. Note how the `summarise_if` function is working here; if a variable in the dataset is numeric, then it is summarized by calculating its `mean`.  
```{r}
ed_data %>% 
  group_by(Trt_rand) %>% 
  summarise_if(is.numeric, mean) %>% 
  select(-c(ID, Trt_non_rand))
```
Both the treatment and control groups appear to be approximately the same on average on the observed characteristics of gender, minority status, SES, and pre-treatment reading and math scores. Note that since `FEMALE` is coded as 0 - 1, the "mean" is simply the proportion of students in the dataset that are female. The same is true for `MINORITY`.

In our hypothetical randomized experiment, after randomizing students into the treatment and control groups, we would then implement some educational intervention. For example, perhaps students in the treatment group recieve some new specialized curriculum, and the students in the control group receive the "business-as-usual" curriculum. We would then measure student reading and math scores again at the end of the year, and if we observed that the treatment group was scoring higher (or lower) on average than the control group, we could attribute that difference entirely to the new curriculum. We would not have to worry about other omitted variables being the cause of the difference in test scores, because randomization ensured that the two groups were equivalent on average on *all* pre-treatment characteristics, both observed and unobserved. 

In comparison, in an observational study, the two groups are not equivalent on these pre-treatment variables. In the same example above, let us imagine where instead of being randomly assigned to treatment, instead students with lower SES are assigned to the new specialized curriculum (T = 1), and those with higher SES are assigned to the business as usual curriculum (T = 0). To accomplish this in R, let's create a second version of our `ed_data`, which we will call `ed_data_obs`, and use the `arrange()` function to sort the dataset by `SES_CONT`. We can then assign the 167 students with the lowest SES (who are now in the first 167 rows) a value of 1 for the treatment group and the 168 students with the highest SES to a value of 0 for the control group.

In this case, the table of comparisons between the two groups looks quite different:  
```{r}
ed_data %>% 
  group_by(Trt_non_rand) %>% 
  summarise_if(is.numeric, mean) %>% 
  select(-c(ID, Trt_rand))
```
There are somewhat large differences between the treatment and control group on several pre-treatment variables (e.g. % minority, and reading and math scores). Notice that the two groups still appear to be balanced in terms of gender. This is because gender is in general not associated with SES. However, minority status and test scores are both correlated with SES, so assigning treatment based on SES (instead of via a random process) results in an imbalance on those other pre-treatment variables. Therefore, if we observed differences in test scores at the end of the year, it would be difficult to disambiguate whether the differences were caused by the intervention or due to some of these other pre-treatment differences.

## If you know Z, what about multiple regression?

In the previous sections, we made clear that you cannot calculate the causal effect of a treatment using a *simple* linear regression model unless you have random assignment. What about a *multiple* regression model?

The answer here is more complicated. We’ll give you an overview, but note that this is a tiny sliver of an introduction and that there is an entire *field* of methods devoted to this problem. The field is called **causal inference methods** and focuses on the conditions under and methods in which you can calculate causal effects in observational studies. 

Recall, we said before that in an observational study, the reason you can’t attribute causality between X and Y is because the relationship is **confounded** by an omitted variable Z. What if we included Z in the model (making it no longer omitted), as in:

$$\hat{y} = b_0 + b_1T + b_2Z$$
As we learned in Chapter \@ref(multiple-regression), we can now interpret the coefficient $b_1$ as **the estimated effect of the treatment on outcomes, holding constant (or adjusting for) Z**.

Importantly, the relationship between T and Y, adjusting for Z can be similar or different than the relationship between T and Y alone. In advance, you simply cannot know one from the other.

We can verify that this did result in the treatment group scoring approximately 10 points higher than the control group (since all students gain an average of 20 points during the year).


```{r scores-rand, echo=FALSE, message=FALSE}
if(!file.exists("data/scores_rand_exp.csv")){
  scores_rand_exp <- "https://docs.google.com/spreadsheets/d/e/2PACX-1vSMVxtjUVz55lZR-JPgjWRJbxO45WGhTOfP2nXxh2RSGDRgDgmfJcLUSz3k4UZfe2R_kunyj3aezhwv/pub?gid=2115421031&single=true&output=csv" %>%
    read_csv(na = "")
    write_csv(scores_rand_exp, "data/scores_rand_exp.csv")
} else {
  scores_rand_exp <- read_csv("data/scores_rand_exp.csv")
}
```

```{r, message = FALSE}
scores_rand_exp <- read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vSMVxtjUVz55lZR-JPgjWRJbxO45WGhTOfP2nXxh2RSGDRgDgmfJcLUSz3k4UZfe2R_kunyj3aezhwv/pub?gid=2115421031&single=true&output=csv")

glimpse(scores_rand_exp)
```

```{r}
ed_data <- left_join(ed_data, scores_rand_exp)
ed_data %>% 
  group_by(Trt_rand) %>% 
  summarise(post_trt_rand_avg = mean(MATH_post_trt_rand))
```

Remember that in a randomized experiment, we calculate the treatment effect by simply taking the difference in the group averages (i.e. $\bar{y}_T - \bar{y}_C$), so here our estimated treatment effect is $69.8 - 59.3 = 10.5$. Recall that we said this could be estimated using the simple linear regression model $\hat{y} = b_0 + b_1T$. We can fit this model in R to verify that our estimated treatment effect is $b_1 = 10.5$.

```{r}
fit <- lm(MATH_post_trt_rand ~ Trt_rand, data = ed_data)
fit
```

Let's also look at post-treatment test scores in our `ed_data_obs` data set as well for the non-randomized experiment case, and look at two regression models estimating the effect of treatment on math scores, with and without controlling for SES. 

```{r scores-non-rand, echo=FALSE, message=FALSE}
if(!file.exists("data/scores_non_rand_exp.csv")){
  scores_non_rand_exp <- "https://docs.google.com/spreadsheets/d/e/2PACX-1vT3NhXRiKpfX4h6uAcPin3gpUhILaLOfPK8KMen1_mujayYOKXME16vC8NikKM6PK8IFaN_jAc6TzD6/pub?gid=111711059&single=true&output=csv" %>%
    read_csv(na = "")
    write_csv(scores_non_rand_exp, "data/scores_non_rand_exp.csv")
} else {
  scores_non_rand_exp <- read_csv("data/scores_non_rand_exp.csv")
}
```

```{r, message = FALSE}
scores_non_rand_exp <- read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vT3NhXRiKpfX4h6uAcPin3gpUhILaLOfPK8KMen1_mujayYOKXME16vC8NikKM6PK8IFaN_jAc6TzD6/pub?gid=111711059&single=true&output=csv")

glimpse(scores_non_rand_exp)
```
```{r}
ed_data <- left_join(ed_data, scores_non_rand_exp)
fit1_non_rand <- lm(MATH_post_trt_non_rand ~ Trt_non_rand, data = ed_data)
summary(fit1_non_rand)$coefficients

fit2_non_rand <- lm(MATH_post_trt_non_rand ~ Trt_non_rand + SES_CONT, data = ed_data)
summary(fit2_non_rand)$coefficients
```

The two models give quite different indications of how effective the treatment is. In the first model, the estimate of the treatment effect is `r summary(fit1_non_rand)$coefficients[2,1]`, but in the second model once we control for SES, the estimate is `r summary(fit2_non_rand)$coefficients[2,1]`. This is because here treatment status is confounded with SES. 

Importantly, in the randomized experiment case, controlling for confounders using a multiple regression model is **not** necessary - again, because of the randomization. Let's look at the same two models using the data from the experimental case (i.e. using `ed_data_rand_exp`). 
```{r}
fit1_rand_exp <- lm(MATH_post_trt_rand ~ Trt_rand, data = ed_data)
summary(fit1_rand_exp)$coefficients

fit2_rand_exp <- lm(MATH_post_trt_rand ~ Trt_rand + SES_CONT, data = ed_data)
summary(fit2_rand_exp)$coefficients
```
We can see that both models give estimates of the treatment effect that are roughly same (`r summary(fit1_rand_exp)$coefficients[2,1]` and `r summary(fit2_rand_exp)$coefficients[2,1]`), regardless of whether or not we control for SES. This is because randomization ensured that the treatment and control group were balanced on all pre-treatment characteristics - including SES, so there is no need to control for them in a multiple regression model. 

## What if you don’t know Z?

In the observational case, if you *know* the process through which people are assigned to or select treatment then the above multiple regression approach can get you pretty close to the causal effect of the treatment on the outcomes. This is what happened in our `fit2_obs` model above where we knew treatment was determined by SES, and so we controlled for it in our model. 

But this is **rarely** the case. In most studies, selection of treatment is **not based on a single variable**. That is, before treatment occurs, those that will ultimately receive the treatment and those that do not might differ in a myriad of ways. For example, students that play instruments may not only come from families with more resources and have higher motivation, but may also play fewer sports, already be great readers, have a natural proclivity for music, or come from a musical family. As an analyst, it is typically very difficult – if not impossible – to know how and why some people selected a treatment and others did not. 

Without randomization, here is the best approach:

1.	Remember: your goal is to approximate a random experiment. You want the two groups to be similar on any and all variables that are related to uptake of the treatment and the outcome.
2.	Think about the treatment selection process. Why would people choose to play an instrument (or not)? Attend an after-school program (or not)? Be part of a sorority or fraternity (or not)? 
3.	Look for variables in your data that you can use in a multiple regression to control for these other possible confounders. Pay attention to how your estimate of the treatment impact changes as you add these into your model (often it will decrease). 
4.	State very clearly the assumptions you are making, the variables you have controlled for, and the possible other variables you were unable to control for. Be tentative in your conclusions and make clear their limitations – that this work is suggestive and that future research – a randomized experiment – would be more definitive. 

## Conclusion
In this chapter we’ve focused on the role of randomization in our ability to make inferences – here about causation. As you will see in the next few chapters, randomization is also important for making inferences from outcomes observed in a sample to their values in a population. But the importance of randomization goes even deeper than this – one could say that **randomization is at the core of inferential statistics**. 

In situations in which treatment is **randomly assigned** or a sample is **randomly selected** from a population, as a result of **knowing this mechanism**, we are able to imagine and explore alternative realities – what we will call **counter-factual thinking** (Chapter \@ref(sampling)) – and form ways of understanding when “effects” are likely (or unlikely) to be found simply by chance – what we will call **proof by stochastic contradiction** (Chapter \@ref(pvalues)). 

Finally, we would be remiss to end this chapter without including this XKCD comic, which every statistician loves:

![](images/causation.png)


